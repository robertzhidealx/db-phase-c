[ Partners ]

- Jiaxuan Zhang, jzhan239
- Jessie Luo, jluo30

[ Issues ]

In terms of generating and cleaning our data sets, there are mainly two issues that we had to particular tackle. One is that we had to select and fetch data ourselves from the various APIs. There are no readily made CSV files and the like, so we had to pick interesting repositories, packages, etc. that are meaningful ourselves and fetch them from the API. Also, we had to orchestrate the various relationships between entities by matching on IDs, etc. The second issue is how to make our data correlate with each other such that it satisfies foreign key constraints. For example, IssueCreator is a relation between entity User and Issue, and it has has foreign key constraints on both. To build the connection, we have to first iterate over Issue to collect creator info, then add the creator with its userID, login, url, and type to user.txt if it wasn’t in it already. This ensures each issueID for IssueCreator is in issue.txt and the person who created it is one of our users. Of course, all of this is done via scripting.

When loading txt files to SQL, we noticed that we needed to make some necessary adjustments to our txt file for it to be conform to the database format. The data we get directly from API values contained special characters like non-English symbols, which the database cannot recognize. Besides, we are using commas to separate fields in each tuple as in csv format. When importing files, we saw warnings like “row contained more data than there were input columns.” Turned out some strings themselves contained comma(s) that splat them into one or multiple attribute values. Thus, we wrote a regular expression pattern to remove these erratic symbols from the attribute values, which effectively fixed both issues.
